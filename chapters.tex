\chapter{Introduction}\label{introduction}
Dialogue system (DS) is a computer system which interacts with a human in natural language. These systems are used in cars (hands-free car-specific functions, Android Auto, Apple CarPlay), web, robots, computer games etc, because a conversation is a natural way for people to get information. 

Natural Language Generation (NLG) is an important component of dialogue systems. NLG goal is to imitate human behaviour, what is very important for dialogue systems. DS can be classified into task-oriented, which focused on completing a certain tasks and adhere to a determined script for each stage of the conversation, and non-task-oriented, which do not have a stated goal to work towards. A lot of devices have incorporated task-oriented dialogue systems, such as Yandex’s Alisa, Apple’s Siri, Microsoft’s Cortana, Amazon Alexa, and Google Assistant. Task-oriented dialogue acts make conversations more interpetable and controllable. On the other hand, they also hinder scaling such systems to new domains (i.e. conversation topics). To escape from the limitation, recent interest of research started moving to non-task-oriented chitchat dialogues (chatbots). Chitchat dialogues are systems designed to mimic the unstructured human-human conversation. This kind of dialogue systems often have an entertainment value, such as Cleverbot, Microsoft's XiaoIce system etc. Chatbots have also been used for testing theories of psychological counseling.

The ability to communicate freely in a natural language is one of the hallmarks of human intelligence, and is likely one of the requirements for true artificial intelligence. Many researchers work on open-ended (i.e. there is a huge range of appropriate outputs given the input) chitchat dialogues to explore this aspect of intelligence, because in task-oriented dialogue systems there is a relatively narrow range of correct outputs given the input. An example of possible responses to an input sequence is shown in the Figure \ref{dialogue_example}, but there are still many ways to answer the question ``Hi, how are you?''. All answers have different attributes of generated text, such as sentiment, lengths of answers and specificity, which can be controlled.  

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{figures/dialogue_example.pdf}
  \caption{Example of answers to the input sequence with setted attributes.}
  \label{dialogue_example}
\end{figure}

Creating a non-task-oriented DS is a challenge for researchers, because there are a lot of topics of conversations as well as user reactions and responses to them. Such bots often suffer from the inability to generate human-like conversations. Their replies are often too generic, because non-specific responses are in natural language (e.g. ``Ok, I see'', ``I don't know''). 

According to \cite{salovey1990emotional} one of the most important cognitive behaviors in humans is expressing and understanding emotions. That is why it is necessary to pay attention not only to generation of a semantically and syntactically correct text, but also to the emotions and language style in which person communicates to make a dialogue more diverse and interesting. Carefully formulated speech without cliches or jargon is essential to avoid inaccurate presentation and ensure effective communication. Most of the modern generative models are trained on huge corpora which include different contributions from various authors. Texts produced with such models are often not perceived as natural and characterized as non-human, because humans have recognizable writing and communication styles.

The main purpose of this thesis was to create a dialogue system, which is able to generate text in different styles and control the manifestation of each style. Chapter \ref{ds_problems} presents common NLG problems, as well as the specific NLG problems that arise during the dialogue. Chapter \ref{nlg_models} contains the overview of models used in the Natural Language Generation. It starts off with an explanation of fundamental techniques common to a lot of machine learning approaches and then continues to describe models specific for NLG and used in the experiments in this thesis. Chapter \ref{related_work} contains a portfolio of current methods and models for solving the problems described in the chapter \ref{ds_problems}. Chapter \ref{eval_datasets_section} describes evaluation methods for NLG in dialogue systems and  publicly available datasets, which are used for the models training in the thesis. Chapter \ref{solution_design} presents designed solution for stylized natural language generation and its implementation. Chapter \ref{results_discuss} contains the results of the experiments, their analysis and evaluation of the designed model. Chapter \ref{conclusion} summarizes the results reported in the previous chapter and providing recommendations on future research in stylized natural language generation in dialogue systems, which are based on the thesis' results. 

\chapter{Dialogue systems}\label{ds_problems}
According to \cite{alder2017handbook} Natural Language Generation is defined as ``the process by which thought is rendered into language''. NLG approaches can be grouped into two categories, one focuses on generating text using templates or (linguistic) rules (i.e. data-to-text generation), the other uses corpus-based statistical methods (i.e. text-to-text generation), where corpus is a collection of texts \cite{oh2002stochastic}. Spectrum of these approaches is represented in the Figure \ref{nlg_spectum}.


\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/templateVScorpus.pdf}
  \caption{Spectrum of the NLG approaches.}
  \label{nlg_spectum}
\end{figure}

\section{Template-based approach} 

\begin{table}[ht]
\centering
 \begin{tabular}{|c|} 
 \hline
 Example: \\
 \hline
 User's input: \\
 ``I'm going to travel from Moscow on April 2.'' \\ 
 \hline
 Template: \\
 What time would you like to travel from $\{departure\_city\}$ on $\{departure\_date\}?$ \\
 \hline
 Agent's output:\\
 ``What time would you like to travel from Moscow on April 2?'' \\
 \hline
 \end{tabular}
 \caption{The example of template-based approach.}
\label{tab:tb_example}
\end{table}

Until recently Natural Language Generation component of a dialog system used primarily hand-coded generation templates, which represented model sentences in a natural language mapped to a particular semantic content.
The template-based system selects a proper response for the current conversation from a repository with response selection algorithms. Templates are often designed for a specific task in a given domain \cite{manishina2016data}. 
Example of template-based system is shown in the Table \ref{tab:tb_example}.


\subsubsection{Advantages of template-based approach}
The first advantage is that the output produced by this approach is likely to be grammatically correct and not contain unexpected generation errors. The second one is that the process of sentence generation is fully controlled, these models are robust and reliable because they consist of clearly defined rules. 

\subsubsection{Disadvantages of template-based approach}
These models require time and human resources to deploy a real dialogue system, because templates are constructed manually, and the number of templates grows quickly (using different templates for singular and plural versions). The next disadvantage is that human created templates often sound unnatural due to their generic structures. Another weak points are that template-based systems are not able to handle unknown inputs and cannot make variation in output, it is just concatenation of strings. This approach also is not flexible, because it has limits to use templates in other domains. The last one is that a template-based model is not able to learn and is not able to adapt to the user, that's why it generates rigid and stylised responses without the natural variation of human language.

\section{Corpus-based approach}
Corpus-based system dominates the NLG community, special in the case of open-ended tasks, where it is almost impossible to hand-craft the templates for all possible combinations of semantic units. Corpus-based systems include statistical and machine learning approaches to resolve it \cite{rudnicky2002dialog}. This approach mines large datasets of human-human conversations.

\subsubsection{Advantages of corpus-based approach}
Corpus-based models have ability to generate more proper responses that could have never appeared in the corpus; it is possible to mimick the language of a real domain expert and use this models for open-domain dialogue systems; dynamic approach is able to learn and to handle unknown inputs, it is also has a lot of possible variations of output.

\subsubsection{Disadvantages of corpus-based approach}
It is necessary to have a corpus, which contains a large amount of data and on a variety of topics to get a sensible output. Even if you have the corpus, process of text generation is not fully controlled and the output can be incorrect or does not make a sense. This approach still has a lot of problems, what will be described in more detail in the section \ref{nlg_ds_problems}. 

\section{Language Models}
In corpus-based system natural language generation uses \textbf{Language Models(LMs)} to generate sequences of texts. LM is a probabilistic model which learns to predict the probability of a sequence of words. The equation \ref{eq:LM} represents the language model, where $W$ is a sequence and $w_1, w_2, ..., w_n$ are words in this sequence. 

\begin{equation} \label{eq:LM}
P(W) = P(w_1, w_2, ..., w_n)
\end{equation}

The \textbf{Chain rule} (Equation \ref{eq:CR}) calculates the joint probability of a sentence by using the conditional probability (Equation \ref{eq:CB}) of a word given previous words. 
\begin{equation} \label{eq:CR}
P(w_1, w_2,..., w_n) = \prod_{i}P(w_i|w_1, w_2,...,w_{i-1})
\end{equation}

\begin{equation} \label{eq:CB}
P(A|B) = P(A \cap B) / P(B)
\end{equation}
In equation \ref{eq:CB} $P(A \cap B)$ is the probability that both events A and B occur.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/lm.pdf}
  \caption{Example of calculating a sentence probability by using chain rule.}
  \label{chain_rule}
\end{figure}

An example in the Figure \ref{chain_rule} shows how to predict probability of a word given previous words. A subsequence (context) may consist of a very large number of words and the likelihood that such subsequence is found in a corpus is very small. It is a main problem in language models, which is called \textbf{data sparsity}.

Data sparsity is the phenomenon of not observing enough data in a corpus to model language accurately. The solution to resolve this issue is to make the assumption that the probability of a word depends only on the previous \textit{n} words and use \textbf{N-gram model} (N-gram is a sequence of N words).

\begin{table}[ht]
  \centering
   \begin{tabular}{|c|c|} 
   \hline
    Hi & 1-gram \\
   \hline
    New York & 2-gram \\
   \hline
   The Three Musketeers & 3-gram \\
   \hline
   She is studying IT & 4-gram \\
   \hline
   \end{tabular}
   \caption{The example of N-grams.}
  \label{tab:n_gram}
\end{table}

The n-gram ``She is studying IT'' from the Table \ref{tab:n_gram} does not occur as often in texts of corpus as n-grams ``Hi'', ``New York'' and ``The Three Musketeers''. Knowing a probability to the occurrence of an N-gram in a sequence of words can be useful, because it can help to decide which N-grams can be chunked together to from single entities (like ``New York'' chuncked together as one word). It can also help make next word predictions. For example, ``tea'' is more likely than ``ball'' in the phrase ``I would like to drink''.


According to \cite{bengio2003neural} another way to fight with data sparsity is learning a distributed representation for words, which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously a distributed representation for each word along with the probability function for word sequences. A sequence of words that has never been seen before gets high probability if words in this sequence are similar in the sense of having a nearby representation to words forming an already seen sentence. Authors used neural networks (artificial neural networks are described in the section \ref{nn_section}) for the probability function. The proposed approach improved n-gram models and took an advantage of longer contexts.


\section{Problems in dialogue systems}\label{nlg_ds_problems}
The ability to communicate with machines in a natural language is a long-standing dream of a mankind. Today's dialogue systems often encounter criticism. There are many scientific works on creating more natural dialogue systems. Markus M. Berg defines a natural dialogue system in \cite{berg2014modelling} as ``a form of dialogue system that tries to improve usability and user satisfaction by imitating human behaviour''. It affects the features of human-to-human dialogue (for example, topic changes, sub-dialogues) and seeks to integrate them into dialogue systems for human interaction with the machine. Open-ended natural dialogue systems still have flaws in generating a response to the user.

\begin{table}[ht]
\centering
 \begin{tabular}{|p{0.5cm}|p{8cm}|p{4cm}|} 
 \hline
 \textbf{\#} & \textbf{Example} & \textbf{Problem type} \\
 \hline
 1 & While Bob ate an apple when it swims. & adequacy\\
 \hline
 2 & Why a mouse when it spins? & adequacy \\
 \hline
 3 &  -Yes, I'm studying law at the moment. & repetition\\
   & - Good. & \\
   & - I like playing the piano. & \\
   & - Good.  & \\
\hline
4 & -Do you go get coffee often? & response-relatedness \\
  & -I am a musician. & \\
\hline
 \end{tabular}
 \caption{Examples of DS problems.}
\label{tab:ds_probs}
\end{table}
Main problems of dialogue systems are represented in the Table \ref{tab:ds_probs}. A problem of adequacy shows that a response can be grammatically and syntactically composed correctly, but this sentence does not make sense. A problem of repetition makes conversation boring. A problem of response-relatedness shows that the answer to the question does not make a sense in this context and it spoils the impression of the conversation.

As noticed in \cite{stent2005evaluating}, the main task of NLG is to select, inflect and order words ``to communicate the input meaning'' as completely, clearly and fluently as possible in context. That's why it is necessary to control if output is appropriate or felicitous in a given context. A good generator usually relies on several factors:
\begin{itemize}
  \item \textbf{adequacy} (a sentence that is ambiguous or not contains communicates meaning in the input, is \textbf{not} adequate)
  \item \textbf{repetition} (self-repetition across utterances and with utterances, repeating the conversational partner)
  \item \textbf{response-relatedness} (efficacy in context)
  \item \textbf{variation} (there are 2 basic forms of variation: \textit{word choice variation} and \textit{word order variation} for enriching speech)
\end{itemize}

\begin{table}[ht]
\centering
 \begin{tabular}{|p{0.5cm}|p{8cm}|} 
 \hline
 \textbf{\#} & \textbf{Example} \\
 \hline
 1 & I bought movie tickets on Tuesday. \\ 
 \hline
 2 & I got movie tickets on Tuesday. \\
 \hline
 3 & On Tuesday I bought movie tickets. \\
 \hline
 4 & On movie Tuesday tickets I bought. \\
 \hline
 5 & I bought tickets for the Tuesday movie. \\ 
 \hline
 \end{tabular}
 \caption{The example of sentences' variation.}
\label{tab:var_example}
\end{table}

An example in the Table \ref{tab:var_example} shows all types of variation. Sometimes this factor can be syntactically incorrect or unclear, what you can see in the forth sentence. In fifth sentence a variation changed the meaning of part of the sentence. In addition, the variation may add or remove meaning possibilities.


One of the hardest problem in text generation is a language style, which makes a response to an user more human. This task is challenging due to the difficulty of capturing emotional factors and the complex mechanism of human emotions. Some people use obscene speech, some use a lot of expressive means, jargon or jokes to make speech more emotional. This is what distinguishes people and makes their communication more interesting.

\chapter{Concepts of Natural Language Generation} \label{nlg_models}
Development of NLG from templates to dynamic generation of sentences took a lot of time and models developed along with it. Corpus-based generation uses a generative probabilistic model what can be implemented in many ways. The model focuses on response generation in the context of dialogue, where the task is to generate a response, given an input sequence. Thus, these models fit well within the sequence-to-sequence (seq2seq) (i.e. encoder-decoder) models, which are described in more detail in section \ref{seq2seq_section}. Neural networks are explained for a better understanding seq2seq model.

\section{Neural Networks} \label{nn_section}
Artificial Neural Networks are inspired by biological neural networks that constitute animal brains. Artificial neuron (Figure \ref{neuron}) is a computational unit in an artificial neural network with a set of real-valued inputs $x_1, x_2 ... x_n$ and an output $y$, where each input $x_i$ has a corresponding weight $w_i$. Weights determine the influence of the input on the output. The neuron's output is the weighted sum of its inputs, which are passed through a non-linear function known as an activation function or transfer function. The transfer functions usually have a sigmoid shape and model the threshold for neuron firing. Bias is an additional parameter in the neural network, which is used to adjust the output along with the weighted sum of the inputs to the neuron. Bias value allows to shift the activation function.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/ai_neuron.pdf}
  \caption{Architecture of an artificial neuron.}
  \label{neuron}
\end{figure}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.3\textwidth, height=0.2\textheight]{figures/nn.pdf}
  \caption{The fully-connected neural network with 1 hidden layer.}
  \label{nn}
\end{figure}

Neurons' outputs can be connected to inputs of other neurons and the calculation is propagated through the network. In NNs neurons are organized into layers where generally the output of the layer is the input for a next layer. The Figure \ref{nn} represents the most common type of layer -- the fully-connected layer where all neurons between adjacent layers are connected with each other.

\section{Elman Networks} \label{rnn_section}
Elman Networks are a form of recurrent neural networks (RNN) used in natural language processing (NLP) as they allow modeling temporal depedencies of variable length in the data (like context) to be captured. RNNs share the same structure as NNs described in the section \ref{nn_section}, except each layer also has an internal state (i.e. hidden state), which captures information about the previous layer inputs. This allows the network to keep track of past data while processing current inputs.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/rnn.pdf}
  \caption{Architecture of Elman neural network, where variable \textbf{h} is a hidden state, \textbf{x} is an input and \textbf{o} is an output. \textbf{w} is a weight, which is optimised to produce a sensible output.}
  \label{rnn}
\end{figure}

The architecture of Elman Networks is illustrated in the Figure \ref{rnn}. A hidden state \textbf{h} is a vector, which calculated from the input $x$ and the previous hidden state. An output \textbf{o} is calculated from this new hidden state. 
The equations \ref{eq:rnn_h} and \ref{eq:rnn_o} show the formulas for a traditional recurrent neural network.

\begin{equation} \label{eq:rnn_h}
h_t = \sigma(W_hh_{t-1} + W_xx_t)
\end{equation}

\begin{equation} \label{eq:rnn_o}
o_t = softmax(W_oh_t)
\end{equation}

The RNN-based models have been used for NLG as a component of end-to-end trainable task-oriented dialogue system \cite{wen2016network}. 

Nowadays traditional RNN networks almost are not used in NLG, because they have problems with vanishing and exploding gradients. As introduced in \cite{bengio1994learning} the exploding problem refers to the large increase in the spectral norm of the Jacobian matrix during training. It happens, because long term components can grow exponentially more then short term ones. The vanishing gradients problem refers to the opposite behaviour. The long term components go exponentially fast to spectral norm 0, which makes it impossible for the model to learn correlation between temporally distant events. This issue has motivated researchers in development of more advanced RNNs like the LSTM \cite{hochreiter1997long}.

\section{Long short-term memory (LSTM)} \label{lstm_section}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/lstmCell.pdf}
  \caption{A cell in an LSTM network.}
  \label{lstm}
\end{figure}

LSTM networks are a special kind of RNN, which reduce the vanishing gradient problem. It makes them much more effective on capturing long-term dependencies. All recurrent neural networks have the form of a chain of repeating modules of neural network. LSTM network also has this chain structure, but the repeating module has a different structure. The key of the solution is usage of multiple gates and a cell state, which runs through all the cells and is manipulated using these gates – parts of the state may be added or removed. Each gate is a sigmoid layer that outputs a number between 0 and 1, which represents the degree of the cell state modification.

The Figure \ref{lstm} represents a structure of a LSTM cell. LSTM cell processes data sequentially for 1 time step and keeps its hidden state through time. First, the network decides how much of information from previous steps to keep stored in its cell state, by using the forget gate, which consists of a sigmoid function applied to weighted sum of previous output, input and bias (Equation \ref{eq:lstm_1}). $W$ are updated through the backpropagation algorithm weights, $b_f$ is a bias, $x_t$ is an input, $h_{t-1}$ is a hidden state from previous step.
\begin{equation} \label{eq:lstm_1}
f_t = \sigma(W_{xf}x_t + W_{hf}h_{t-1}+b_f)
\end{equation}

The next step is to decide how much of the inner state is going to be updated (i.e. what part of the result the cell is going to store in its state), by using input gate (equation \ref{eq:lstm_2}).
\begin{equation} \label{eq:lstm_2}
i_t = \sigma(W_{xi}x_t + W_{hi}h_{t-1}+b_i)
\end{equation}

After calculating the state modification, it is necessary to compute the new values (i.e. candidate values) which will be stored in it, by using activation function (equation \ref{eq:lstm_3}).
\begin{equation} \label{eq:lstm_3}
\tilde{c_{t}}=tanh(W_{xc}x_t + W_{hc}h_{t-1} + b_c)
\end{equation}

Updating the cell state is based on the previous state and the candidate values (Equation \ref{eq:lstm_4}).
\begin{equation} \label{eq:lstm_4}
c_t = \tilde{c_{t}} \odot i_t + c_{t-1} \odot f_t
\end{equation}

Output gate is represented in equation \ref{eq:lstm_5}.
\begin{equation} \label{eq:lstm_5}
o_t = \sigma(W_{xo}x_t + W_{ho}h_{t-1}+b_o)
\end{equation}

And the final step producing the hidden state for the next timestep. It is based on the newly updated cell state, transformed by tanh function and multiplied by the output gate (equation \ref{eq:lstm_6}).
\begin{equation} \label{eq:lstm_6}
h_t = o_t \odot tanh(c_t)
\end{equation}

This model does not suffer from vanishing gradient, but still the capacity of the LSTM memory is limited and high computational requirements make learning LSTM difficult. 

\section{Sequence-to-sequence model (seq2seq)} \label{seq2seq_section}

Seq2seq models were introduced in 2014 \cite{sutskever2014sequence}. This model uses an encoder-decoder architecture (Figure \ref{encoder_decoder}). Both the encoder and the decoder are recurrent neural networks (vanilla version of RNN is rarely used, because of the problems described in the section \ref{rnn_section}). The role of the encoder is to encode the input, a sequence of variable length data, to a fixed length vector. Decoder based on this vector generates an output sequence of data of different length. These 2 neural networks are connected into one model to maximise the learning effect. Seq2seq model is very effective to solve NLP problems, because input and output sequences can have different lengths and recurrent neural networks can work with context. This model is often used in machine translation, text summarization, dialogue systems etc.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/encoder_decoder.pdf}
  \caption{Architecture of sequence-to-sequence model. $h_i$ represents a hidden state.}
  \label{encoder_decoder}
\end{figure}

The Figure \ref{encoder_decoder} presents traditional encoder-decoder arhitecture. Encoder converts an input sequence of words to a corresponding fixed size hidden vector. Each vector represents the current token and the context of it. Every time step, it takes a vector that represents a word and pass its output to the next layer. The last hidden state of encoder passes its output to the first layer of the decoder. The final hidden state of the encoder is also called context vector. The decoder input is an output encoder vector and start token, which characterizes the beginning of the generated sentence. The generated word depends on the previous decoder state and the last generated word. Many improvements have led to other seq2seq components, such as attention, beam search, bucketing.

\section{Attention} \label{attention_section}
A neural attention mechanism is based on the human visual attention mechanism. Visual attention is able to focus on a certain region of an image with ``high resolution'', while perceiving the surrounding image in “low resolution”, and then adjusting the focal point over time.

The attention mechanism provides the decoder with information from each hidden state of the encoder and it gives a model the ability to selectively focus on useful parts of the input sequence and learn the alignment between them (Figure \ref{fig:attention}).

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.5\textwidth]{figures/attention_example.png}
  \caption{An example of attention mechanism. The x-axis and y-axis correspond to the words in the input sequence in English and the generated translation in French. Each pixel represents a weight.}
  \label{fig:attention}
\end{figure}

\begin{eqfloat}
\begin{equation} \label{eq:attention}
output_i = \sum_{j=1}^{T} \alpha_{ij} h_j
\end{equation}
\caption{An output of the attention mechanism, where $T$ is a length of input sequence, $h_j$ is a hidden state representing a value, $\alpha_{ij}$ (Equation \ref{eq:attention_w}) is a weight of each annotation $h_j$.} 
\end{eqfloat}

\begin{eqfloat}
\begin{equation} \label{eq:attention_w}
\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T} exp(e_{ik})}
\end{equation}
\caption{Computation of $\alpha_{ij}$ of each annotation $h_j$, where $e_{ij}$ is an alignment model, represented as feedforward neural network, which scores how well match the inputs around position $j$ and the output at position $i$.} 
\end{eqfloat}

In \cite{vaswani2017attention} attention is described as ``mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors''. The output represents a computed weighted sum of the values (Equation \ref{eq:attention}). A weight for each value is computed by a compatibility function of the query with the corresponding key. Weights for keys corresponding to important values will be greater than for unimportant ones, which means that important values will be a bigger part of the output. 

Self-attention is an attention mechanism, where ``self'' means that the inputs interact with each other and ``attention'' means that inputs find out who they should pay more attention. Formally, in the attention mechanism the query, keys and values are from the same sequence. The query is a single element from the sequence while the keys and values are the entire sequence. The attention output is a new representation of the element that was the query. Self-attention is used to compute a new representation of the sequence.  

\section{Transformer} \label{sec_transformer}
Information in this section is taken from \cite{vaswani2017attention}.
\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.9\textwidth]{figures/transformer_attention.pdf}
  \caption[Transformer architecture]{The architecture of the Transformer.\protect\footnotemark}
  \label{transformer}
\end{figure}

Transformer introduces an architecture that is based on self-attention mechanism and does not use any recurrent networks. In each step this model applies self-attention mechanism which directly models relationships between all words in a sequence, regardless of their respective position. Transformers do not require that the sentence be processed in order, that allows process parallelization during training, unlike RNN. Due to this feature, it has enabled training on much more data. 

The architecture of the Transformer is illustrated in the Figure \ref{transformer}. The Transformer consists of a stack of encoders (on the left) for processing inputs of any length and another set of decoders (on the right) to output the generated sentences. $N_x$ in the Figure means that modules of encoder and decoder can be stacked on top of each other multiple times. Modules consist of multi-head attention and feed forward layers. The inputs and output are first embedded into an n-dimensional space. Words' positions are added to the embedded representation, n-dimensional vector, of each word.
\footnotetext{\url{http://primo.ai/index.php?title=Transformer}}

\begin{equation} \label{eq:transformer_attention}
Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V
\end{equation}

\begin{equation} \label{eq:transformer_multi_head}
MultiHead(Q, K, V) = Concat(head_1, head_2, ..., head_h)W^O
\end{equation}

An Equation \ref{eq:transformer_attention} represents a \textbf{scaled dot-product attention}. The input consists of values of dimension $d_v$, queries and keys of dimension $d_k$, where queries, keys and values are matricies.

An Equation \ref{eq:transformer_multi_head} represents a \textbf{multi-head attention}, which allows the model to jointly track information from different representation subspaces at different positions. Averaging inhibits this with a single head of attention. The input also consists of queries, keys and values matricies, $W^O$ is a parameter matrix, $h$ is a number of parallel layers, $head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$, where $W_i^Q, W_i^K, W_i^V$ are parameter matricies. $Q$, $K$ and $V$ are different for each position of the attention modules in the structure. It depends on if they are in the encoder, the decoder or between them.

\section{GPT} \label{sec_gpt}
Information in this section is taken from \cite{radford2018improving}.
GPT is a transformer-based (Section \ref{sec_transformer}) language model. The model works in 2 stages. First of all transformer model trained on a very large amount of data in an unsupervised manner (There are only input variables and no correspoding output variables in unsupervised dataset.), then the model is fine-tuned on much smaller supervised dataset (There are input and output variable in supervised dataset. The model learns the mapping function from the input to the output.) to help it solve specific tasks.

\subsubsection{Unsupervised pre-training}
In unsupervised pre-training a standard language modeling is used and the aim is to maximize the likelihood (Equation \ref{eq:unsupervised_pre_training}, where $U = {u_1, u_2, ... , u_n}$ is an unsupervised corpus of tokens, $k$ is the size of context window, $P$ is modeled using a neural network with parameter $\Theta$).

\begin{equation} \label{eq:unsupervised_pre_training}
L_1(U) = \sum_i \log P(u_i|u_{i-k}, ... , u_{i-1}; \Theta)
\end{equation}

Multi-layer Transformer decoder is used for the language model. The model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution
over target tokens (Equation \ref{eq:unsupervised_decoder}, where $U = (u_{-k}, ... , u_{-1})$ is the context vector of tokens, $n$ is the number of layers, $W_e$ is the token embedding matrix, $W_p$ is the position embedding matrix).

\begin{equation}  \label{eq:unsupervised_decoder}
\begin{array}{lcl} 
h_0 & = &UW_e + W_p \\

h_l & = & transformer\_block(h_{l-1}) \forall l \in [1,n] \\

P(u) & = & softmax(h_n, W_e^T) \\
\end{array}
\end{equation}

\subsubsection{Supervised fine-tuning}
After training the model, the parameters are adapted to the supervised target task. Given a labeled dataset $C$, where an instance represents a sequence of input tokens $(x^1, ... , x^m)$, along with a label $y$. The inputs are passed through the pre-trained model to get the final transformer block's activation $h_l^m$, which is then fed into an additional linear output layer with $W_y$ parameters for predicting $y$ (Equation \ref{eq:supervised_fine_tune}). 

\begin{equation} \label{eq:supervised_fine_tune}
P(y|x^1, ... , x^m) = softmax(h_l^m, W_y)
\end{equation}

\begin{equation} \label{eq:supervised_maximize}
L_2(C) = \sum_{(x,y)} \log P(y|x^1, ... , x^m)
\end{equation}

Authors found that including language modeling as an auxiliary objective to the fine-tuning helped learning. First of all it improves the generalization of the supervised model. Secondly it accelerates convergence. They optimize the objective with weight $\lambda$ (Equation \ref{eq:objective_optimalization}).

\begin{equation} \label{eq:objective_optimalization}
L_3(C) = L_2(C) + \lambda * L_1(C)
\end{equation}

\section{BART} \label{bart_section}
BART is a denoising autoencoder for pretraining sequence-to-sequence models, that maps a corrupted document to the original document it was derived from. The model uses a standard Transformer-based neural machine translation architecture, with a bidirectional encoder and left-to-right decoder.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/BART.pdf}
  \caption{BART: Inputs to the encoder need not be aligned with decoder outputs, allowing arbitary noise transformations. Here, a document has been corrupted by replacing spans of text with mask symbols. The corrupted document (left) is encoded with a bidirectional model, and then the likelihood of the original document (right) is calculated with an autoregressive decoder. For fine-tuning, an uncorrupted document is input to both the encoder and decoder, and we use representations from the final hidden state of the decoder\cite{lewis2019bart}.}
  \label{bart}
\end{figure}

Bidirectional encoder consists of $N$ transformer encoder blocks stacked on top of each other (Section \ref{sec_transformer}), the final block is the output. The input is a sequence of tokens, which are first embedded into vectors and the processed in the model. The output is a sequence of vectors of size $H$, in which each vector corresponds to an input token with the same index. Bidirectional encoder uses 2 training strategies: \textbf{Masked LM (MLM)} and \textbf{Next Sentence Prediction (NSP)}. 

In \textbf{MLM} 15\% of words in each sequence are replaced with a [MASK] token before embedding. The model then tries to predict the original value of the masked words, based on the context provided non-masked words in the sequence. Bidirectional encoder loss function takes into consideration only the prediction of masked values. Pretraining on this task allows the model to learn general features of the language.

\textbf{Next Sentence Prediction}. The bidirectional encoder in the training process receives pairs of sequences as input and the task of the model is to say whether the second sentence is a subsequent of the first sentence in the original document. During training, part of the inputs are a pair in which the second sentence is a subsequent in the original document and in the other part a random sentence from the corpus is chosen as the second sentence. The assumption is that the random sentence will be disconnected from the first sentence. Pre-training on this task allows the model to learn relationships between sentences.

\begin{eqfloat}
\begin{equation} \label{eq:relu}
f(x) = x^+ = max(0, x)
\end{equation}
\caption{Rectified Linear Unit (ReLU)}
\end{eqfloat}

\begin{eqfloat}
\begin{equation} \label{eq:gelu}
GELU(x) = xP(X \leq x) = x \Phi(x)
\end{equation}
\caption{Gaussian Error Linear Unit (GELU)}
\end{eqfloat}

\begin{eqfloat}
\begin{equation} \label{eq:gelu_approx}
\begin{array}{lcl} 
GELU(x) = 0.5x(1 + tanh[\sqrt{2/\pi}(x + 0.044715x^3)]) \\ 
GELU(x) = x\sigma(1.702x) \\
\end{array}
\end{equation}
\caption{Approximations of GELU}
\end{eqfloat}
Autoregressive decoder is GPT model (Section \ref{sec_gpt}). Authors modified ReLU activation functions (Equation \ref{eq:relu}) to GELU (Equation \ref{eq:gelu}) and initialized parameters from $\mathcal{N}(0, 0.2)$. Autoregressive decoder can be directly fine tuned for sequence generation tasks.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.6\textwidth]{figures/bart_doc_corruptions.pdf}
  \caption{Transformations for noising the input. These transformations can be composed.}
  \label{bart_doc_corruption}
\end{figure}

BART allows to apply any type of document corruption, such as token masking, sentence permutation, document rotation, token deletion, text infilling (Figure \ref{bart_doc_corruption}).

\chapter{Related work} \label{related_work}
This chapter presents an overview of the most popular NLG models for building open-ended dialogue systems and for resolving problems described in the section \ref{nlg_ds_problems}. Section \ref{related_work_ds_section} presents approaches for resolving standard NLG problems. Section \ref{stylized_problems} presents solutions for generating stylized text, which helps to create the feeling that you are talking to a person.

\section{Dialogue system problems} \label{related_work_ds_section}
Modeling conversations with generative probabilistic models was first proposed in \cite{ritter2011data}. They present a data-driven approach to generating responses to Twitter status posts, based on
phrase-based Statistical Machine Translation. In \cite{shang2015neural} authors proposed a framework for generating responses on micro-blogging websites with using recurrent neural networks. Their model can generate grammatically correct and content-wise appropriate responses to over 75\% of the input text.

\subsubsection{Text generation methods}
In \cite{see2019makes} solutions to common NLG problems in dialogue systems are described. Authors add control (the ability to specify desired attributes of the generated text at test time) and focus on four controllable attributes of text: repetition, specificity, response-relatedness and question-asking. They measure repetitiveness as n-gram overlap, specificity as word rareness, response-relatedness as the embedding similarity of the bot’s response to the human’s last utterance. In this work, authors use \textbf{Conditional Training (CT)} \cite{peng2018towards} and \textbf{Weighted Decoding (WD)} \cite{ghazvininejad2017hafez}. 

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/ct.pdf}
  \caption{Example of Condition Training model.}
  \label{ct}
\end{figure}


A \textbf{CT} model learns probabilities $P(y|x,z)$, where $y$ is the output text, $x$ is the input text and $z$ is a control variable, which specifies the desired output attribute. In the model $z$ is presented with learned embedding and is concatenated to each decoder input (Figure \ref{ct}). For example, to get very generic or very specific response, $z$ can be set to LOW or HIGH. If it is necessary simultaneously control several attributes, multiple control embeddings ($z_1, z_2, ..., z_n$) can be concatenated and the model learns $P(y|x, z_1, z_2, ..., z_n)$. Disadvantage of Conditional Training is that it can’t control attributes without sufficient training data. CT model learns only a very weak connection between $z$ and the semantic relatedness of the output.

A \textbf{WD} is a technique applied during decoding to increase/decrease the probability of words with certain features. 
\begin{equation} \label{eq:wd}
score(w, y_{<t}; x) = score(y_{<t}; x) + \log P_{RNN}(w|y_{<t}, x) + \sum_i w_i * f_i(w; y_{<t}, x)
\end{equation}
In weighted decoding (Equation \ref{eq:wd}), a hypothesis $y_{<t} = y_1, ..., y_{t-1}$ is expanded by computing the score for each possible next word $w$ in the vocabulary on the $t^{th}$ step of decoding. $\log P_{RNN}(w|y_{<t}, x)$ is the log-probability of the word $w$ calculated by the RNN. $score(y_{<t}; x)$ is the accumulated score of the already-generated words in the hypothesis $y_{<t}$. $f_i(w; y_{<t}, x)$ is a decoding feature with associated weights $w_i$ (hyperparameters to be chosen). Each feature presents a specific controlling attribute.

\begin{equation} \label{eq:nidf}
NIDF(w) = \frac{IDF(w) - min\_idf}{max\_idf - min\_idf}
\end{equation}
\textbf{Normalized Inverse Document Frequency (NIDF)} is used as a measure of word rareness. In condition training the metric is represented as $z$ variable and in weighted decoding as a parameter of rareness for a word prediction (On each step of the decoding, the probability of each word in the vocabulary is updated in proportion to its rareness. The size of the update is controlled by a weight parameter.) (Equation \ref{eq:nidf}). $IDF(w) = \log(\frac{R}{c_w})$ is a Inverse Document Frequency of a word $w$, where $R$ is the number of responses in the dataset, $c_w$ is the number of those responses that contain $w$. $min\_idf$ and $max\_idf$ are the minimum and maximum IDF's, which are used for normalising the NIDF (ranges from 0 to 1).

Question-asking problem is resolved in CT by setting the variable $z$ to 1 of 11 possible values: $\{0, 1, ..., 10\}$, where $z=i$ means that the model should produce, on average, utterances containing ``?'' with probability $\frac{i}{10}$. In weighted decoding the binary decoding feature is used, which is equal to 1 if a word $w$ is in a pre-defined list of words: ``how, what, when, where, which, who, whom, whose, why, ?'', 0 otherwise.

N-gram based decoding features were defined to control repetition with WD. External (self-repetition across utterances), internal (self-repetition within utterances) and partner (repeating the conversational partner) repetition fetures identify repeating bigrams. Other features identify repeating content words. Negative weight is applied to these features to reduse repetition. 
\begin{equation} \label{eq:response_relatedness}
resp\_rel(w; y_{<t}, x) = cos\_sim(word\_emb(w), sent\_emb(l)) 
\end{equation}
Response-relatedness feature with weighted decoding is represented in the Equation \ref{eq:response_relatedness}, where $word\_emb(w)$ is the GloVe embedding for the word $w$, $sent\_emb(l)$ is the sentence embedding for the partner's last utterance $l$ ($l$ is part of the context $x$), $cos\_sim$ is the cosine similarity between two.
The control response-relatedness in condition training is represented by variable $z$ as $cos\_sim(sent\_emb(y), sent\_emb(l))$, where $cos\_sim$ is a cosine similarity between the model's response $y$ and the partner's last utterance $l$.

\subsubsection{Decoding strategies}
According to \cite{holtzman2019curious} decoding strategies with likelihood maximazing lead to text that is increadibly degenerate, even when using state-of-the-art models. If the most likely word is always sampled, the model generates repetitive and overly generic text, like ``I don't know'', because it is a typical answer to any question.

Maximization-based decoding methods such as \textbf{beam search} make text incoherent and repetitive. A beam search is a limited-width breadth first search. This method starts from an empty sequence $(t=0)$, at every step $t=0, 1, 2, 3,...$ beam search expands at most $k$ partial sequences (with highest probabilities) and computes the probabilities of sequences with length $t+1$. It terminates with a beam of $k$ complete sequences. 

Popular sampling methods for generation texts are based on sampling from the distribution. Temperature and top k sampling are the most popular methods to combat sampling from the tail.

\textbf{Temperature sampling} is inspired by statistical thermodynamics, where high temperature means low energy states are more likely encountered. In a probability model logits are divided by the temperature, before feeding them into softmax (Equation \ref{eq:temperature_sampling}, where $t$ is a temperature, $u_{1:|V|}$ are logits). Setting $t \in [0,1)$ skews the distribution towards high probability events, which implicitly lowers the mass in the tail distribution. The temperature parameter controls the shape of distribution without sufficiently suppressing the unreliable tail.

\begin{equation} \label{eq:temperature_sampling}
p(x=V_l|x_{1:i-1}) = \frac{exp(u_l / t)}{\sum_{l'} exp(u_l' / t)}
\end{equation}

\textbf{Top-k sampling} means sorting by probability and probabilities for anything below the token $k$ are set to 0. But in some cases, there are few words to choose, there is a risk of generating bland or generic text, while if $k$ is large the top-k vocabulary will include inappropriate candidates which will have their probability of being sampled increased by the renormalization. The top-k vocabulary $V^{(k)} \subset V$ (the set of size $k$) maximizes $p' = \sum_{x \in V^{(k)}} P(x|x_{1:i-1})$. The distribution is then re-scaled as in Equation \ref{eq:top_K_sampling}.

\begin{equation} \label{eq:top_K_sampling}
P'(x|x_{1:i-1}) = \begin{cases} 
          P(x|x_{1:i-1}) / p' & \mbox{if $x \in V^{(k)}$} \\
          0 & \mbox{otherwise}\\
         \end{cases}
\end{equation}

The research shows how different natural distribution of human texts and the distribution of machine text produced from maximum likelihood decoding. To resolve this problem authors introduced \textbf{Nucleus Sampling}. The concept is that the vast majority of probabilities are concentrated in a small subset (\textit{nucleus}) of the vocabulary that tends to vary from one to a few hundred candidates. Sampling from the top-$p$ portion of the probability mass expands and contracts the candidate pool dynamically. Formally, given a distribution $P(x|x_{1:i-1})$, the top-p vocabulary $V^{(p)} \in V$ is defined to satisfy the condition in the Equation \ref{eq:nucleus_sampling}.

\begin{equation} \label{eq:nucleus_sampling}
\sum_{x \in V^{(p)}} P(x|x_{1:i-1})  \geqslant p
\end{equation}

\section{Generating stylized conversation} \label{stylized_problems}
The idea that computers can generate stylized texts already appeared half a century ago in \cite{wheatley1965computer}. Authors choose a variety of text characteristics as style, such as an expession of politeness in machine translation \cite{sennrich2016controlling}, transformation from modern English to Shakespearean English \cite{jhamtani2017shakespearizing}, sentiment of a text in \cite{shen2017style} and \cite{li2018delete}. In \cite{gao2019structuring} authors used a structured latent space to generate stylized dialogue responses. Another approach was proposed in \cite{john2018disentangled}, where authors applied an adversarial loss to separate style from content. The problem of style transfer, described in these papers, differs from the stylized text generation, because as it is shown in \cite{guu2018generating} an existing human-written source used to control the reliability of the results can significantly improve the quality of the resulting texts. 

\subsubsection{ECM}
Emotional Chatting Machine (ECM), represented in \cite{zhou2018emotional}, can generate not only relevant and grammatical responses, but also emotionally consistent. This framework proposes seq2seq architecture. This separate responses into several categories (Angry, Disgust, Happy, Like, Sad, Other). The emotion category of the to-be-generated response is given for ECM, because, according to the authors, emotions are highly subjective. 

\begin{figure}[hbt]
  \centering
  \includegraphics[width=1\textwidth]{figures/ecm.png}
  \caption{ Overview of ECM (the grey unit). The pink units are used to model emotion factors in the framework.}
  \label{ecm}
\end{figure}

In the architecture (Figure \ref{ecm}) a post can be answered with different emotions, depending on the attitude of the respondent. For example, for a sad story, someone may respond with anger  (as an irritable stranger), sympathy (as a friend) or happy (as an enemy). \textbf{Emotion classifier} generates labels for each response. Generated labels and responses are fed into ECM to generate emotional responses conditioned on different emotion categories. In \textbf{Emotion Category Embedding} randomly initialize the vector of an emotion category $v_e$ for each category $e$. During the training the model is learning the vectors of the emotion category. \textbf{Internal memory} captures emotion dynamics during decoding. It is achieved using the following concept: before the decoding process each category has an internal emotion state; at each step the emotion state decays by a certain amount; when decoding process is completed, the emotion state should be zero, what indicates that the emotion is completely expressed. \textbf{External memory} is used to model emotion expressions explicitly, the model can choose to generate words from  an emotion vocabulary or a generic vocabulary.

ECM need an external decision maker, because this model has to specify an emotion category to be generated.

\subsubsection{Generation of stylized texts}

In the paper \cite{tikhonov2018guess} authors describe the problem of stylized text generation in a multilingual setup and show the importance of phonetics for generating the author's stylized poetry. They presented a version of a language model based on a LSTM artificial neural network with extended phonetic and semantic embeddings for stylized poetry generation. The model generates texts resembling the writing style of a particular author.

\begin{equation} \label{eq:poetry_nlg}
G(C|S) = \begin{cases} 
          (C, \mathbb{R}^m, F) \rightarrow \{T_i^G\} \\
          \{T_i^G|S\}  \mathtt{\sim}  \{T_i|S\} & \mbox{w.r.t. D}\\
         \end{cases}
\end{equation}

The equation \ref{eq:poetry_nlg} represents the stylized model, where $C = \{ T_i\}^M_{i=0}$ is a corpus of $M$ literary texts written in one natural language. Every text of length $l$ is a sequence $T_i = (w_j)^l_{j=0}$ ($w_j$ is a word). Words are drawn from a vocabulary set $V = \{ w_j \}^L_{j=1}$, $L$ is a vocabulary size. $(C, \mathbb{R}^m, F)$ is all information available to us.

Performance metric $D$ usually tries to minimize $D(\{T_i\}, \{T_i^G\})$, where $\{T_i^G\}$ is a randomized sample of $C$. $S$ is a subset of continuous and categorical variables out of $(\mathbb{R}^m, F)$ and metric $D$. Artificial neural networks are used for language modeling to avoid the dimensionality curse by effective mapping $(C,\mathbb{R}^m, F) \rightarrow \mathbb{R}^d$ and then train the model like that $G(C):\mathbb{R}^d \rightarrow \{ T_i^G\}$. 

Advantage of this model is a customization. To control cetain parameters of the model, it is needed to include them intp $S$. The output $\{T_i^G|S\}$ will resemble original texts $\{T_i|S\}$ that satisfy $S$ conditions. The name of an author of a poetic text is used as a condition $S$ for the model. 

Another key feature of the proposed model is a concatenated word representation, where one of the LSTMs works with letters from char-representation of the word and another one uses phonemes of the International Phonetic Alphabet, employing an heuristics to transcribe words into phonemes.
 
\chapter{Evaluation and datasets} \label{eval_datasets_section}
\section{Evaluation methods}\label{eval}
The majority of NLG researches published between 2005-2014 relies on automatic metrics \cite{gkatzia2015snapshot}. A lot of NLG evaluations use automatic metrics, because it is a cheap and fast method. These metrics are reasonable if they are known to be enough correlated with human preferences. According to \cite{artstein2009semi} conversational dialogue systems cannot be evaluated by automatic metrics, because dialogue is heavily dependent on context and theory of current dialogue is not precise enough to predetermine the target output.  

Metrics METEOR \cite{banerjee2005meteor}, BLEU \cite{papineni2002bleu}, ROUGE \cite{lin2004rouge} are usually used for automatic summarization. They assume that valid responses have significant word overlap with the ground truth responses. In dialogue systems these metrics cannot be used, because there is significant diversity in the space of valid responses to a given context \cite{liu2016not}.

In \cite{novikova2017we} the weak correlation between human and automatic evaluations is also confirmed. Authors compared different word-based (\textbf{WBM} relies on ground-truth references) and grammar-based (\textbf{GBM} does not rely on ground-truth references) metrics. Their model, combination of WBMs and GBMs, achieved high correlation with humans but only within a single domain.

Corpus is very important for successful Natural Language Generation. Dialogue systems require training data in the format of people text conversation, for example, non-fiction or movie reviews are not suitable for this. Large volumes of training data improves the decision-making ability of NLG model, so those models can use it to figure out patterns. Unfortunately, there are not a lot of datasets available for training NLG models, due to the high cost of creating quality datasets.  

\section{Persona-Chat dataset} \label{persona_chat_subsec}

\begin{table}[ht]
  \begin{tabular}{ |c|c| }
   \hline
   \textbf{Persona 1} & \textbf{Persona 2}  \\ 
   \hline
    I like to ski. & I am an artist. \\
    My wife does not like me anymore. & I have four children. \\
    I have went to Mexico 4 times this year. & I recently got a car. \\
    I hate Mexican food. & I enjoy walking for exercise. \\
    I like to eat cheetos. & I love watching Game of Thrones. \\
    \hline
    \multicolumn{2}{l}{[PERSON 1:] Hi} \\
    \multicolumn{2}{l}{[PERSON 2:] Hello! How are you today?} \\
    \multicolumn{2}{l}{[PERSON 1:] I am good thank you, how are you.} \\
    \multicolumn{2}{l}{[PERSON 2:] Great, thanks! My children and I were just about to watch Game of Thrones.} \\
    \multicolumn{2}{l}{[PERSON 1:] Nice! How old are your children?} \\
    \multicolumn{2}{l}{[PERSON 2:] I have four that range in age from 10 to 21. You?} \\
    \multicolumn{2}{l}{[PERSON 1:] I do not have children at the moment.} \\
    \multicolumn{2}{l}{[PERSON 2:] That just means you get to keep all the popcorn for yourself.} \\
    \multicolumn{2}{l}{[PERSON 1:] And Cheetos at the moment!} \\
    \multicolumn{2}{l}{[PERSON 2:] Good choice. Do you watch Game of Thrones?} \\
    \multicolumn{2}{l}{[PERSON 1:] No, I do not have much time for TV.} \\
    \multicolumn{2}{l}{[PERSON 2:] I usually spend my time painting: but, I love the show.} \\
    \hline
  \end{tabular}
  \caption{Example of a dialogue from the Persona-Chat dataset \cite{zhang2018personalizing}.}
\label{tab:persona_chat}
\end{table}

\begin{table}[t]
\centering
  \begin{tabular}{ |c|c| } 
   \hline
   \textbf{Original Persona} & \textbf{Revised Persona}  \\ 
   \hline
   I love the beach. & For me, there is nothing like a day at the seashore. \\ 
   My dad has a car dealership. & My father sales vehicles for a living.  \\
   I just got my nails done. & I love to pamper myself on a regular basis. \\
   I am on a diet now. & I need to lose weight. \\
   Horses are my favorite animal. & I am into equastrian sports. \\ 
   \hline
  \end{tabular}
  \caption{Example of original and revised personas \cite{zhang2018personalizing}.}
\label{tab:persona_revised}
\end{table}

\begin{table}[t]
\centering
  \begin{tabular}{|p{10cm}|p{1cm}|} 
  \hline
  Average number of words in first persona description: & 6.332 \\
  \hline
  Average number of words in second persona description: & 6.321 \\
  \hline
  Average number of words in the first person's utterances: & 11.419 \\
  \hline
  Average number of words in the second person's utterances: & 11.929 \\
  \hline
  Number of first persona description's sentences: & 40239 \\
  \hline
  Number of second persona description's sentences: & 40126 \\
  \hline
  Number of the first person's utterances: & 65719 \\
  \hline
  Number of the second person's utterances:  & 65719 \\
  \hline
  Number of dialogues & 8938 \\
  \hline
  \end{tabular}
  \caption{Persona-Chat statistics.}
\label{tab:persona_chat_statistics}
\end{table}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/persona_desc.pdf}
  \caption{Histogram of persona description lengths.}
  \label{fig:histogram_persona_desc}
\end{figure}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/uttr_length.pdf}
  \caption{Histogram of sequences' lengths of a person.}
  \label{fig:histogram_uttr_length}
\end{figure}

Persona-Chat models normal conversation when 2 people meet for the first meet and try to get know each other better. The aim of the dialogue is to learn about interests of another person, find common ground and discuss their hobbies. The task involves both asking and answering questions. 

Persona-Chat dataset consists of small conversations between 2 crowdworkers from Amazon Mechanical Turk who were randomly paired and asked to act the part of a given provided persona (randomly assigned, and created by another set of crowdworkers). The data collection consists of persona chat (each dialogue has 6-8 turns), personas (set of 1155 possible personas, each consisting of at least 5 profile sentences), revised personas to avoid word overlap, because crowdworkers sometimes could repeat profile information in a chat(the Table \ref{tab:persona_revised}). In turn-based dialogue each message consists of a maximum of 15 words. All statistics are presented in the Table \ref{tab:persona_chat_statistics}, the Figure \ref{fig:histogram_persona_desc} and the Figure \ref{fig:histogram_uttr_length}. An example of Persona-Chat dialogue is shown in the Table \ref{tab:persona_chat}. The dataset contains 262,848 message-responses pairs and splitted into train data -- 70\%, valid data -- 20\% and test data -- 10\%.

\section{Datasets for stylisation and pretraining} \label{stylistic_dataset_section}
\textbf{Stanford Sentiment Treebank (SST)} contains sentences from movie reviews and includes labels for every syntactically plausible phrase in thousands of sentences. This corpus represents sentiment of reviews. To create SST the corpus of movie review excerpts from the paper \cite{pang2005seeing} is used, where HTML tags and sentences that are not in English are deleted from this dataset. The Stanford Parser is used to parse all 10,662 sentences and some snippet were splitted into multiple sentences. The resulting 215,154 phrases were labeled by Amazon Mechanical Trunk. There are 25 different labels from \textit{very negative} to \textit{very positive}. Annotators most often used only 5-class classification: negative, somewhat negative, neutral, positive or somewhat positive. Many of sentences could be considered neutral, therefore, to generate text with positive and negative sentiment I have used negative and somewhat negative classes as one negative class and positive, somewhat positive classes as one positive class. A histogram for SST is represented in the Figure \ref{fig:sst}.

For poetic style the \textbf{Shakespeare} dataset was used, which contains all of Shakespeare's plays. A histogram for this dataset is shown in the Figure \ref{fig:shakespeare}.

\begin{table}[ht]
\centering
 \begin{tabular}{|p{4cm}|p{3cm}|p{3cm}|} 
 \hline
 reddit\_jokes.json & 195K jokes & 7.40M tokens \\
 \hline
 stupidstuff.json & 3.77K jokes & 396K tokens \\
 \hline
 wocka.json & 10.0K jokes & 1.11M tokens \\
 \hline
 TOTAL & 208K jokes & 8.91M tokens \\
 \hline
 \end{tabular}
 \caption{Statistics of jokes' dataset.}
\label{tab:jokes}
\end{table}


A dataset of \textbf{english plaintext jokes} was used for generating text with humor style. There are about 208 000 jokes scraped from 3 sources (Table \ref{tab:jokes}). stupidstuff.json is scrapped from stupidstuff.org, wocka.jsom from \url{http://wocka.com}, reddit\_jokes.json is scraped from \url{https://www.reddit.com/r/Jokes} and contains all submissions to the subreddit as of 13.02.2017. A histogram for the dataset is shown in the Figure \ref{fig:jokes}.

\textbf{Twitter} dataset contains 867,710 message-response pairs from Twitter and is used for pretraining baseline model. The dataset is splitted into train data -- 85\%, valid data -- 10\% and test data -- 5\%. A histogram for the dataset is shown in the Figure \ref{fig:tweet}


\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/sst.pdf}
  \caption{Histogram of Stanford Sentiment Treebank labeled reviews.}
  \label{fig:sst}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth]{figures/shakespeare.pdf}
  \caption{Histogram of Shakespeare's plays.}
  \label{fig:shakespeare}
\end{figure}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/jokes.pdf}
  \caption{Histogram of jokes lengths.}
  \label{fig:jokes}
\end{figure}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.7\textwidth]{figures/tweet.pdf}
  \caption{Histogram of tweets' lengths.}
  \label{fig:tweet}
\end{figure}
\chapter{Design and implementation} \label{solution_design}
This chapter describes the model for stylized text generation in dialogue systems and performed experiments. All experiments can be divided into 3 parts: decoding strategies, feature-based decoding modifications and weighted decoding of a combination of language models. The designed architecture for WD of a language models combination is represented in the Figure \ref{architecture_nlg} and implemented in two variants. First, a LSTM-based baseline model was implemented and trained. The second variant uses state-of-the-art models. All parts of the architecture were implemented in Python 3.7.3 with using PyTorch 1.4.0 framework.

\begin{figure}[hbt]
  \centering
  \includegraphics[width=0.8\textwidth]{figures/model.pdf}
  \caption{An architecture of combination models for stylized Natural Language Generation. $w_n$ is a hyperparameter.}
  \label{architecture_nlg}
\end{figure}

\section{Baseline}
Encoder-decoder model in baseline is implemented as LSTM sequence-to-sequence language model, represented in the chapter \ref{nlg_models}, with Luong attention, described in the appendix \ref{luong_attn_appendix}. This model is pretrained on message-response pairs from Twitter dataset (subsection \ref{stylistic_dataset_section}) and trained on the Persona-Chat dataset, described in the subsection \ref{persona_chat_subsec}. As the input $x$ to the encoder-decoder model, the entire dialogue history, separated by unique token, is used.

Stylized language model is implemented as LSTM-based RNN model. Each stylized language model is trained separately on the corresponding stylistic dataset described in the subsection \ref{stylistic_dataset_section}.

Data are tokenized, where tokenization is a process of splitting text into units represented at model's input and replacing sensitive data with unique identification symbols that retain all the essential information about the data without compromising its security. Spacy\protect\footnotemark library is used with few handcrafted rules for tokenization. As input GloVe (Global Vectors) embeddings are used for distributed word representation \cite{pennington2014glove}. It is an algorithm, which maps words into a meaningful space where the distance between words is related to semantic similarity. The model was trained only on the nonzero elements in a word-word cooccurrence matrix. Outputs of encoder-decoder and stylized language models (probabilities of words) are multiplied by weights (weight is a hyperparameter to be chosen) to increase or decrease the probability of words and finally these outputs are added up (Figure \ref{implementation_architecture}).

\footnotetext{\url{https://spacy.io/}}

\begin{figure}[hbt]
  \centering
  \includegraphics[width=1\textwidth]{figures/combine_models.pdf}
  \caption{Stylized Natural Language Generation baseline model.}
  \label{implementation_architecture}
\end{figure}

\begin{eqfloat}
\begin{equation} \label{eq:cross_entropy}
H(p,q) = -\sum_{x \in X} p(x)\log q(x)
\end{equation}
\caption{$p$ is the target distribution, $q$ is the approximation of the target distribution. $p(x)$ is the probability of the event $x$ in $p$, $q(x)$ is the probability of the event $x$ in $q$.}
\end{eqfloat}

For all experiments with the baseline 2 hidden layers were used in both the encoder and decoder with 512 hidden unites in each hidden layer. A dropout \cite{srivastava2014dropout} was used with the probability of $50\%$ to prevent neural networks from overfitting. \textit{Cross-entropy} (measure of the difference between 2 probability distributions for a given random variable or set of events) was used as a loss function. Formally representation of cross-entropy is in the Equation \ref{eq:cross_entropy}. \textit{Adam} \cite{kingma2014adam} is used as the optimization algorithm.

\section{Pre-trained models}
BART and GPT-2 models are used to explore all features of the created architecture (Figure \ref{architecture_nlg}). BART is an autoencoder for pretraining sequence-to-sequence models, described in the section \ref{bart_section}. BART model with 400M hyperparameters has 12 encoder and decoder layers. This model is chosen because it achieves state-of-the-art results on Conv AI2 task (a dialogue response generation task, conditioned on context and a persona). GPT-2 is a large transformer-based language model, which was trained to predict the next word in 40GB of Internet text. GPT-2 with 117M hyperparameters has large improvements on WikiText2 dataset for language modeling. This model is a successor to GPT, described in the section \ref{sec_gpt} and used as stylized language model. 

\begin{table}[ht]
\centering
 \begin{tabular}{|p{5cm}|p{3cm}|p{3cm}|} 
 \hline
 \textbf{Model name} & \textbf{Valid F1} & \textbf{Valid PPL} \\
 \hline
 Seq2Seq + Attention & 16.02 & 35.07 \\
 \hline
 BART & 20.72 & 11.85 \\
 \hline
 \end{tabular}
 \caption{BART outperforms previous work on conversational response generation. Results are from \cite{lewis2019bart}.}
\label{tab:bart_statistic}
\end{table}


BART decoder is an equivalent to the GPT model. GPT-2 is a direct scale-up of GPT, with more parameters (1.5 billion parameters) and trained on more amount of data. GPT-2 model uses more words in a vocabulary than GPT, what means that before a combination of these models, it is necessary to convert a distribution of probabilities of the next word in a sequene over all the words in the smallest vocabulary to the distribution over the biggest one and add 0 if word does not exist. Outputs of BART model, transformed to GPT-2 vocabulary, and GPT-2 models (probabilities of words) are multiplied by weights to increase or decrease the probability of words and finally these outputs are added up.

Each GPT-2 model represents one style. 4 such models are fine-tuned for humor, positive and negative sentiment and poetic style. The model is trained with batch size parameter set to 1, learning rate for Adam is 0.0001, 50000 chars is a minimum size to concatenate input files with $<|endoftext|>$ separator. 

BART is fine-tuned on Persona-Chat dataset. Learning rate is set to 0.00003, label smoothing is 0.1, dropout is also 0.1.

\section{Experiments}
This section represents designed experiments with different features during test time of the designed models to generate stylized dialogues.

The first experiment performed on the BART model uses Beam search and Nucleus sampling methods. These decoding strategies, described in detail in the section \ref{related_work_ds_section}, allow the generation of less generic and repetitive sequences, which affects the user's perception of the text.

The following experiments with feature-based decoding modifications are performed using seq2seq models (BART and LSTM-based seq2seq). The first one is with Normalized Inverse Document Frequency, described in the section \ref{related_work_ds_section}. NIDF is used as a decoding feature, because this approach makes generated text more specific. NIDF represents a measure of word rareness and on each step of the decoding, the probability of each word in the vocabulary is updated in proportion to its rareness. Another experimet is with a length of generated sequences, because a person's style is represented not only by words, but also by the length of sentences. Some people are used to writing very briefly, and someone likes to paint every detail in a text. The last experiment with feature-based decoding modification is blocking stop words. During the decoding useless data are filtered. In NLP useless data are referred to as stop words (a commonly used words, for example, ``a'', ``the'', ``I'' etc.). This approach could help to generate more unusual sequences, for example, instead of ``I think'' to use the phrase ``In my opinion''. 

Next experiment combines sequence-to-sequence model and stylized language models as it is shown in the Figure \ref{architecture_nlg}. 
\begin{equation} \label{eq:baseline}
p(y_1, ... , y_{T'}|x_1, ... , x_T) \propto \prod_{t=1}^{T'} [w_1 * p_1(y_t|v, y_1, ... , y_{t-1}) + \sum_{i=2}^n w_i * p_i(y_t| y_1, ... , y_{t-1})]
\end{equation}

The Equation \ref{eq:baseline} is a formal description of the experiment with weighted decoding, where $x_1, ... , x_T$ is an input sequence and $y_1, ... , y_{T'}$ is its corresponding output sequence whose length $T'$ may differ from $T$. $p_1(y_t|v, y_1, ... , y_{t-1})$ is a distribution for encoder-decoder language model, in which $v$ is the last hidden state of the encoder. $p_i(y_t|v, y_1, ... , y_{t-1})$ is a distribution for each stylized language model, $n$ is a number of stylized models. All distributions are represented with a softmax over all the words in the vocabulary and multiplied by a corresponding weight $w$, a hyperparameter to be chosen.

In last experiment $n$ words in a sequence are generated over encoder-decoder model and $n$ words are generated over stylized language model, where $n$ is a hyperparameter. Sequence generation stops when the end mark of the sequence is generated or the length of the sequence is equal to the maximum length. 

\section{Evaluation}
There are a lot of researches, which shows that human and automatic evaluations have weak correlation (Subsection \ref{eval}). In dialogue systems there are a lot of ways how to answer to the opponent utterance correctly, that's why in this thesis only human evaluaton is used. 

\textbf{A/B testing} \cite{kohavi2017online} is used as model evaluation. This method is described as the randomized assignment to the user of two variants: the \textit{control} and the \textit{treatment}. The control variant typically is the existing version, which in this case it is a real dialogue of 2 people. The treatment variant is the new version being evaluated -- in this case it is a generated dialogue. 

The experiment evaluated several manifestations of human styles, such as sentiment (positive, negative), poetry, humor, and specificity. Users were given a questionnaire with several questions (``Which dialogue is more positive?'', ``Which dialogue is more negative?'', ``Which dialogue is more poetic?'', etc.) and for each question there were 2 dialogs as answer options. One of the dialogs was generated by the model, and the second was from real life. In the assessment, 21 people took part in the age range from 1991 to 2004 with different levels of education and field of activity.

\begin{eqfloat}
\begin{equation} \label{eq:hypergeometric_distribution}
p_X(k) = Pr(X=k) = \frac{\binom{K}{k} \binom{N-K}{n-k}}{\binom{N}{n}}
\end{equation}
\caption{Hypergeometric distribution, where $N$ is the population size, $K$ is the number of success states in the population, $n$ is the quantity drawn in each trial, $k$ is the number of observed successes. $\binom{n}{k} = \frac{n\,!}{k\,!(n-k)\,!}$ is a binomial coefficient.}
\end{eqfloat}

\textbf{Fisher's exact test} \cite{fisher1922interpretation} is carried out to check if human evaluation is statistically significant. This test is used when there are 2 nominal variables (nominal variables classify observations into discrete categories) to know if the proportions for one variable differ among the values of another variable. In this case it is controlled if the order of the dialogs affects the user. 
Fisher's exact test is used when the total sample size is less than 1000. For this test a null hypothesis is put forward. The null hypothesis is that the relative proportions of one variable are independent of the second variable. To confirm or refute this hypothesis, it is necessary to calculate the probability of getting the observed data, and all data sets with more extreme deviations, under the null hypothesis that the proportions are the same. Hypergeometric distribution (Equation \ref{eq:hypergeometric_distribution}) is used to calculate the probability. If the probability is less than 5\%, the null hypothesis is rejected. For the Fisher's exact test, 2 questionaire were created, where there was only 1 question (``Which dialogue is more poetic?''). In the first case, as a response option, the generated dialogue was first given and then the real dialogue, and in the second, vice versa.

\chapter{Results and discussion} \label{results_discuss}
In this chapter, results of the experiments described in the previous section are presented and their implications are discussed. Human evaluation occurred only for those experiments that are subjectively considered more successful.

All experiments with features, decoding strategies and combination of models are carried out on pre-trained models, because the dialogs generated by the baseline model do not make sense in the context. The example in the Table \ref{tab:baseline_experiment} shows that the model also has a repetition problem, such as repetition of the part of an utterance (``I am''), repetition of phrases (``I am afraid''). These problems can be because of the small amount of data and LSTM neural networks still have a problem to hold a context. In comparison with the baseline model the dialogue generated using the BART model (Table \ref{tab:bart_experiment}) is more logical and diverse.

Results of using beam search and Nucleus sampling methods are shown in the Tables \ref{tab:beam_search} and \ref{tab:nucleus_sampling}. All these decoding strategies are described in detail in the section \ref{related_work_ds_section}.
\begin{table}[ht]
\centering
 \begin{tabular}{|p{14cm}|} 
 \hline
 hi , how it goes \\
 \hline
 i love you. \\
 \hline
 i do not love you. What do you mean? \\
 \hline 
 i am very glad you came to England \\
 \hline 
 that sounds like a good man \\
 \hline 
 i have not heard of you yet. \\
 \hline 
 i have been, i have been. You are welcome, sir \\
 \hline
 \end{tabular}
 \caption{Experiment of Beam search decoding method.}
\label{tab:beam_search}
\end{table}

\begin{table}[ht]
\centering
 \begin{tabular}{|p{14cm}|} 
 \hline
  hi, how is thy name \\
 \hline
  i am an ass, and thou shalt be \\
 \hline
  i am sure you have not seen a man till you \\
 \hline 
  i do not know, Wherefore you speak. \\
 \hline 
 i ride you, And i love it \\
 \hline 
 i am very glad you are come \\
 \hline 
 i have a daughter of old \\
 \hline
 \end{tabular}
 \caption{Experiment of Nucleus sampling decoding method.}
\label{tab:nucleus_sampling}
\end{table}

In Nucleus sampling cumulative distribution (CDF) is computed and cutted off as soon as CDF exceeds P. Determined the set of tokens, to be sampled from, may be very large, which can take a very long time. There is no visible difference in the results, so in other experiments beam search is used because of the time.

\section{Feature-based decoding modifications}
\subsubsection{NIDF feature}
In the experiment the Normalized Inverse Document Frequency is used as a decoding feature. This approach makes generated text more specific, because the less often a word is found in a dataset the more weight is setted for the word probability. 

Example results of using BART model with NIDF is shown in the Table \ref{tab:bart_nidf}. NIDF is multiplied by hand tuned weight to increase the probability of words. The conversation generated by BART model with NIDF uses such specific words as ``discard'', ``award'' and \textbf{64.3\%} of people consider that the generated dialogue is more specific than real one.

\subsubsection{Length feature}
For this experiment 2 lengths of a sequence were chosen: 5 and 20 words. With each iteration, when generating the next word, a small weight was added to the probability of the [EOS] (end of sequence) symbol.

\begin{table}[ht]
\centering
 \begin{tabular}{|p{7cm}|p{7cm}|} 
 \hline
   hi, how are you tonight? & hi, how are you?\\
 \hline
  i am a chef. i love to cook. & i am a chef. \\
 \hline
  i do not like carrots either..... & i do not like carrots\\
 \hline 
  i am very athletic. i ride my bike to work every morning & i am very athletic\\
 \hline 
 i am from the country of birth & i am from texas\\
 \hline
 \end{tabular}
 \caption{Experiment of generated sequences with setted length.}
\label{tab:set_len}
\end{table}

Generated sequences with this feature are shown in the Table \ref{tab:set_len}. In some cases, when generating long sentences, many punctuation marks were added, as in the sentence ``i do not like carrots either.....''. \textbf{71.4\%} people consider, that dialogues generated with a lot of words in a sentence are more interesting.

\subsubsection{Stopword blocking}
Stop words are used from NLTK\protect\footnotemark (suite of libraries and programs for symbolic and statistical NLP). During decoding in each iteration, when selecting the next word in the sequence, it is checked if this word is not contained in stop words.

\footnotetext{\url{https://www.nltk.org/}}
\begin{table}[ht]
\centering
 \begin{tabular}{|p{10cm}|} 
 \hline
  good morning \\
 \hline
  oh yeah \\
 \hline
  oh yeah \\
 \hline
  oh nice sound \\
 \hline 
  awwwwwwwwwwwwwww \\
 \hline
 \end{tabular}
 \caption{Experiment of generated sequences with blocking stop words.}
\label{tab:stop_words}
\end{table}
Represented results of the experiment in the Table \ref{tab:stop_words} shows that generated utterances are very generic and don't make sense in a context. For this experiment, there was no human evaluation.

\section{Weighted decoding of combination BART and GPT-2 models}
Results of this experiment are generated dialogues from BART model and one GPT-2 model for stylization, the weights for other stylistic language models are set to 0.

Example of a generated dialogue with humor is shown in the Table \ref{tab:combo_bart_gpt2_jokes}. The weight for encoder-decoder model is set to 0.3, the weight for stylized language model is set to 0.7. These hyperparameters were manual tunned. Compared to the generated dialogue using only the BART model (Table \ref{tab:bart_experiment}), this conversation is not very different, although, for example, the phrase: ``i am a chef for carrots'' may seem rather funny, because the opponent does not like carrots and \textbf{57.14\%} people consider that this dialogue is comical.

Generated dialogue with poetic style is represented in the Table \ref{tab:poetic_shakespear}. Hyperparameters for weighted decoding are set to 0.2 for BART and 0.8 for GPT-2 model. In the example a poetic appeal to a person (``My Lord'') is used, which means a manifestation of style. \textbf{64.3\%} people consider that this dialogue is more poetic than the real one.

In the Table \ref{tab:sst_wd} GPT-2 is trained on SST dataset. As a result generated sentences are short. It could be because sentences in the courpus are ranging from 10 to 20 words (a histogram of the SST dataset is in the Figure \ref{fig:sst}). The negative and positive moods are not expressed in the example. Generated dialogues were compared with the real conversation and in case of positive sentiment only \textbf{21.4\%} people consider that generated dialogue is positive, in case of negative sentiment -- \textbf{50\%} people think that generated conversation is more negative than the real one. 

General conclusion from this experiment is that the more weight was set on generating the style, the more the context began to get lost. Generated sentences were often unrelated in context. Therefore, the following experiment was conducted with the alternate use of models.

\section{Switching BART and GPT-2 models}
In this experiment, alternating the generation of words by the encoder-decoder and stylized language models. $n$ words in a sequence are generated over BART and $n$ words are generated over GPT-2 model, where $n$ is a hyperparameter. Sequence generation stops when the end mark of the sequence is generated or the length of the sequence is equal to the maximum length.

Example of the experiment is represented in the Table \ref{tab:jokes_switch}. When generating sequences, there is still a problem of context, but already the model sometimes generates answers to the opponent's utterance (``-i love carrots but I hate carrots too. -i don't like it either.''). \textbf{71.4\%} people consider that this dialogue is more comic than the real one.

The sentiment and poetic style are not expressed in the examples shown in the Tables \ref{tab:shakespeare_switch}, \ref{tab:sst_switch}. For the conversation generated by the model fine-tuned on the Shakespeare dataset only \textbf{37.5\%} of people consider this dialogue poetic. \textbf{33.3\%} is a human evaluation for negative sentiment and \textbf{23.8\%} is for positive sentiment.

\section{Evaluating the evaluation}
Fisher's exact test is carried out to check if human evaluation is statistically significant, with the help of which the independence of the dialogue from the order of its display is checked. 2 questionnaires are created for the test. In the first one a dialogue with poetic style is displayed as first and real conversation is shown as a second dialogue after that the participant chooses a more poetic dialogue. The second questionnaire is almost the same, only the order in which dialogs are displayed is changed.

The Table \ref{tab:fisher} represents results of the responses of the questionnaire. If the generated dialogue was shown first, then out of 7 people only 4 people chose it as a more stylish dialogue. If the real dialogue was shown first, then out of 8 people, only 3 people chose the generated conversation as a more stylish one.


\begin{table}[ht]
\centering
 \begin{tabular}{|p{4cm}|p{4cm}|p{3cm}|p{2cm}|} 
 \hline
  & Generated dialogue & Real dialogue & Total\\
 \hline
  Generated dialogue first & 4 & 3 & 7 \\
 \hline
  Real dialogue first & 3 & 5 & 8 \\
 \hline
  Total & 7 & 8 & 15\\
 \hline
 \end{tabular}
 \caption{Results from questionaires.}
\label{tab:fisher}
\end{table}

\begin{table}[ht]
\centering
 \begin{tabular}{|p{4cm}|p{4cm}|p{3cm}|} 
 \hline
  & Generated dialogue & Real dialogue \\
 \hline
  Generated dialogue first & 3.267 & 3.733 \\
 \hline
  Real dialogue first & 3.733 & 4.267 \\
 \hline
 \end{tabular}
 \caption{Expected values.}
\label{tab:expected_vals}
\end{table}

To use Fisher's exact test, it is necessary that no more than 80\% of the expected values are more than 5. For the experiment this condition is satisfied (Table \ref{tab:expected_vals}).

\begin{equation} \label{eq:hypergeome_distrib_ex}
p = \frac{\binom{7}{4} \binom{8}{3}}{\binom{15}{7}} = \frac{\binom{7}{3} \binom{8}{5}}{\binom{15}{8}} = 0.305
\end{equation}

In the Equation \ref{eq:hypergeome_distrib_ex} the probability for null hypothesis is represented. In this case it is not necessary to calculate extreme deviations because the probability is already more than 5\%, so null hypothesis is confirmed, that there is no bias between the arrangement of dialogs and the choice of users.

\chapter{Conclusion} \label{conclusion}
The goal was to create a model that generates a stylistic response to the utterance of an opponent in the dialogue, in order to give the impression that the proposals are generated by one person.

In this thesis I have focused on several aspects of style manifestation: specificity, sentiment, humor, poetry. Balance between specificity and genericness is one of the important factors that helps make the generated text more diverse. The mood of the interlocutor is also a key factor in human-human conversation. Each person has emotions, and a neutral dialogue gives a human the feeling that he is talking to a bot. Poetry and humor are a manifestation of high intellectual abilities that are inherent only to man.

For these purposes, an architecture of the model was created that combines several language models. For this model transformer-based and LSTM-based architectures were used. The experiments were conducted not only with the created model, but also with different feature-based modifications during the test time for controlling specificity. Generated sequences by decoding modifications with sentences lengths and word rareness were evaluated as more specific. Experiments with weighted decoding and combinations of models have shown that the more weight is set for a model generating stylistic text and the less weight is set for a model generating text depending on input, the less a dialogue makes sense. This is due to the fact that stylistic datasets do not contain dialogs and stylistic language models generate text regardless of input. In the case of alternate use of models, the text becomes more reasonable, but the style is no longer expressed so clearly.


A potential continuation of this work could collect datasets that contain stylistic dialogues rather than expressions independent of each other. If these datasets are created, it will be possible to use stylistic language models with seq2seq architecture.